{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Optimisation Coursework\n",
    "\n",
    "**Authors: Mark van der Wilk & Luca Grillotti**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose of this coursework\n",
    "\n",
    "The main purpose of this coursework is to get a better understanding of how Gaussian Processes and Bayesian Optimisation work.\n",
    "\n",
    "Consequently this coursework has been divided in 3 dependent parts:\n",
    "1. **Gaussian Processes**, in which you will reimplement a Gaussian Process regression from scratch.\n",
    "2. **Bayesian Optimisation**, in which you will reimplement a Bayesian Optimisation Procedure.\n",
    "\n",
    "This coursework has been designed like a tutorial, and thus contains both **coding tasks** and **questions**. However, only the coding tasks will be assessed via the `LabTS` system.\n",
    "\n",
    "In all this coursework, we will consider the regression setting:\n",
    "$$ y = f(\\mathbf{x}) + \\epsilon, \\: \\epsilon \\sim \\mathcal{N}\\left( 0, \\sigma_n^2 \\right)$$\n",
    "We will place a Gaussian Process prior on $f$ with a mean function $m(\\cdot)=0$ and a covariance kernel function $k(\\cdot, \\cdot)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "# Objective Functions\n",
    "\n",
    "First of all, let's have a look at some functions we are going to work on to test our Gaussian Process regressions and Bayesian Optimisations in parts $1$ and $2$. \n",
    "\n",
    "All the objective functions are stored in the folder `objective_functions/` and all inherit from the abstract class `ObjectiveFunction`. Those classes are already implemented and **should not be modified**.\n",
    "\n",
    "For visualisation purposes, we will exclusively study functions of 1 or 2 variables (except in part 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from acquisition_functions.lower_confidence_bound import LowerConfidenceBound\n",
    "from bayesian_optimisation import BayesianOptimisation\n",
    "from kernels.gaussian_kernel import GaussianKernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Some examples of functions we are going to work on and their noisy versions \n",
    "\n",
    "from objective_functions.six_hump_camel import SixHumpCamelObjectiveFunction\n",
    "from objective_functions.univariate_objective_function import UnivariateObjectiveFunction\n",
    "\n",
    "plt.title(\"Univariate Objective Function\")\n",
    "UnivariateObjectiveFunction().plot([100])\n",
    "\n",
    "plt.title(\"Noisy Univariate Objective Function\")\n",
    "UnivariateObjectiveFunction(additional_gaussian_noise_std=0.5).plot([100])\n",
    "\n",
    "plt.title(\"Six Hump Camel Objective Function\")\n",
    "SixHumpCamelObjectiveFunction().plot([100, 100])\n",
    "\n",
    "plt.title(\"Noisy Six Hump Camel Objective Function\")\n",
    "SixHumpCamelObjectiveFunction(additional_gaussian_noise_std=0.3).plot([100, 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Gaussian Kernel (10 marks)\n",
    "\n",
    "All the kernels we will use are in the folder `kernels/`. Each one of those is represented by a class inheriting from the abstract class `Kernel`.\n",
    "\n",
    "### Task\n",
    "\n",
    "Complete the definition of the function `get_covariance_matrix(X, Y)` in the file `kernels/gaussian_kernel.py`.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "Here `X` and `Y` are two numpy arrays in which each row represents a point at which the objective function could be evaluated.\n",
    "\n",
    "### Return\n",
    "\n",
    "The function should return a numpy array `K` representing the covariance matrix of `X` and `Y`. In this coursework, we will mainly consider the single kernel / covariance function: the squared exponential (Gaussian / radial-basis-function) kernel:\n",
    "\n",
    "$$ K = (k(\\mathbf{x_p}, \\mathbf{y_q}))_{p,q} = \\left( \\sigma_f^2 \\exp\\left( - \\dfrac{1}{2l^2} \\| \\mathbf{x_p} - \\mathbf{y_q}\\|^2 \\right) \\right)_{p,q} $$\n",
    "\n",
    "where: \n",
    "*  $\\sigma_f$ is the amplitude of the latent function\n",
    "*  $l$ corresponds to the length scale.\n",
    "\n",
    "### Useful attributes\n",
    "\n",
    "Each kernel object has at least 3 attributes which you can use to solve this task and the next ones:\n",
    "*  $\\ln{\\sigma_f}$ \n",
    "*  $\\ln{l}$\n",
    "*  and $\\ln{\\sigma_n}$, the logarithm of the noise scale, which will be useful in tasks `2` and `5`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Gaussian Process mean and standard deviation prediction (20 marks)\n",
    "\n",
    "All the gaussian process specific functions are implemented in the `GaussianProcess` class, in the file `gaussian_process.py`.\n",
    "\n",
    "### Task\n",
    "\n",
    "Complete the definition of the function `get_gp_mean_std(new_data_points)` in `gaussian_process.py`.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "The parameter `new_data_points` ($X^*$) is a numpy array  in which all the rows correspond to a data point at which the objective function can be evaluated. Its shape is $n$ x $p$ where: \n",
    "* $n$ is the number of points.\n",
    "* $p$ is the number of coordinates for each point (and the number of variables of the objective function).\n",
    "\n",
    "### Return\n",
    "\n",
    "The function is supposed to return a Tuple of numpy arrays (`mean`, `std`):\n",
    "*  `mean` ($m_{post}(\\cdot)$) - A column vector (of shape $n$ x $1$). Its $i^{th}$ coordinate corresponds to the **posterior** mean estimated by the gaussian process for the $i^{th}$ row of the `new_data_points`.\n",
    "*  `std` ($\\sigma_{post}(\\cdot)$) - a column vector (of shape $n$ x $1$). Its $i^{th}$ coordinate corresponds to the **posterior** standard deviation estimated by the gaussian process for the $i^{th}$ row of the `new_data_points`.\n",
    "\n",
    "### Useful attributes\n",
    "\n",
    "In order to compute those values, you will need to use some of the attributes of the `GaussianProcess` objects:\n",
    "* `_array_dataset` ($X$) - The array of training data (each row is a data point at which the objective function can be evaluated).\n",
    "* `_array_objective_function_values` ($y$) - The array of training data evaluations. It is a column vector whose $i^{th}$ element is the evaluation by the objective function of the $i^{th}$ row of `_array_dataset`.\n",
    "*  `_covariance_matrix` ($k(X,X)$) - the covariance matrix of the training data.\n",
    "*  `_kernel` ($k(\\cdot; \\cdot)$) - the kernel of the gaussian process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "Now, we have:\n",
    "* a kernel, \n",
    "* a function for estimating the mean and standard deviation of a gaussian process. \n",
    "So, we can have a look at some Gaussian Process predictions for some values of $\\sigma_f$, $l$ and $\\sigma_n$.\n",
    "\n",
    "On the graphs below:\n",
    "* The  magenta line represents the predicted mean\n",
    "* The red line represents the computation of mean + 3 * std\n",
    "* The blue line represents the computation of mean - 3 * std\n",
    "* The cyan line represents the (non-noisy) objective function\n",
    "* The green markers '+' represent the dataset evaluations.\n",
    "\n",
    "**If the Gaussian Process does not fit the dataset, it means that the kernel parameters are not appropriate. We will see how to tune them in task 5.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gaussian_process import GaussianProcess\n",
    "from kernels.gaussian_kernel import GaussianKernel\n",
    "\n",
    "objective_function = UnivariateObjectiveFunction(additional_gaussian_noise_std=0.5)\n",
    "\n",
    "kernel_gaussian = GaussianKernel(-1., 0., -1.)\n",
    "gaussian_process = GaussianProcess(kernel_gaussian)\n",
    "\n",
    "gaussian_process.plot(objective_function, show=False)\n",
    "plt.title(\"Gaussian Process Regression - no dataset\")\n",
    "plt.show()\n",
    "\n",
    "boundaries, = objective_function.boundaries\n",
    "x = np.linspace(*boundaries, 50).reshape((-1, 1))\n",
    "y = objective_function.evaluate(x).reshape((-1, 1))\n",
    "gaussian_process.initialise_dataset(x, y)\n",
    "\n",
    "gaussian_process.plot(objective_function, show=False)\n",
    "plt.title(\"Gaussian Process Regression - with dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Function Sampling (10 marks)\n",
    "\n",
    "Technically, as a function contains infinitely many different variables, we cannot sample directly in the distribution of functions. \n",
    "\n",
    "However, we can still *simulate* a function sampling with the Gaussian Process by choosing $n$ points $(x^{(i)}_*)_{i=1..n}$ distributed evenly along the input space. Then we can simulate a \"function sampling\" by sampling using a multivariate normal distribution: \n",
    "$(x_i^*)_{i=1..n} \\sim \\mathcal{N}(\\mu,\\Sigma)$ where:\n",
    "* $\\mu = m_{post}(X_*)$\n",
    "* $\\Sigma = k_{\\text{post}}(X_*, X_*)$\n",
    "\n",
    "### Task\n",
    "\n",
    "Implement a multivariate normal sampling in `get_sample(new_data_points)` in `gaussian_process.py`. \n",
    "\n",
    "### Parameters\n",
    "\n",
    "As before, the parameter `new_data_points` ($X_*$) is a numpy array  in which all the rows correspond to a data point at which the objective function can be evaluated. Its shape is $n$ x $p$ where: \n",
    "* $n$ is the number of points.\n",
    "* $p$ is the number of coordinates for each point (and the number of variables of the objective function).\n",
    "\n",
    "### Returns\n",
    "\n",
    "The function should return a multivariate normal sample of the objective function values. This is a sample from the distribution $\\mathcal{N}(\\mu,\\Sigma)$.\n",
    "\n",
    "-----\n",
    "\n",
    "Once this function is defined you can have a look at the plots of such functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_function = UnivariateObjectiveFunction(additional_gaussian_noise_std=0.5)\n",
    "\n",
    "kernel_gaussian = GaussianKernel(-1., 0., -1.)\n",
    "gaussian_process = GaussianProcess(kernel_gaussian)\n",
    "\n",
    "gaussian_process.plot_with_samples(5, objective_function)\n",
    "plt.show()\n",
    "\n",
    "boundaries, = objective_function.boundaries\n",
    "x = np.linspace(*boundaries, 50).reshape((-1, 1))\n",
    "y = objective_function.evaluate(x).reshape((-1, 1))\n",
    "gaussian_process.initialise_dataset(x, y)\n",
    "\n",
    "gaussian_process.plot_with_samples(5, objective_function)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Matern Kernel (10 marks)\n",
    "\n",
    "The Gaussian Kernel used so far is used for approximating functions which are $C^\\infty$ (having continuous derivatives of all orders). This explains the smoothness of the sampled functions seen before. \n",
    "\n",
    "If we want to use a 1-time differentiable Gaussian Process, we can use the **3/2 Matern Kernel** defined as:\n",
    "\n",
    "$$ k_{Mat, 3/2}(\\mathbf{x}, \\mathbf{y}) = \\sigma_f^2 \\left( 1 + \\dfrac{\\sqrt{3}\\|\\mathbf{x} - \\mathbf{y}\\|}{l}\\right) \\exp\\left(  - \\dfrac{\\sqrt{3}\\|\\mathbf{x} - \\mathbf{y}\\|}{l} \\right) $$\n",
    "\n",
    "----\n",
    "\n",
    "### Task\n",
    "\n",
    "Complete the definition of the function `get_covariance_matrix(X, Y)` in the file `kernels/matern_kernel.py`.\n",
    "\n",
    "\n",
    "### Parameters\n",
    "\n",
    "Here `X` and `Y` are two numpy arrays in which each row represents a point at which the objective function could be evaluated.\n",
    "\n",
    "### Returns\n",
    "\n",
    "The function should return a numpy array `K` representing the covariance matrix of `X` and `Y` for a **3/2 Matern Kernel**.\n",
    "\n",
    "$$ K = (k(\\mathbf{x_p}, \\mathbf{y_q}))_{p,q} $$\n",
    "\n",
    "-----\n",
    "\n",
    "Once this function is defined you can have a look at the plots of functions sampled using this new kernel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_function = UnivariateObjectiveFunction(additional_gaussian_noise_std=0.5)\n",
    "\n",
    "from kernels.matern_kernel import MaternKernel\n",
    "\n",
    "kernel_matern = MaternKernel(-1., 0., -1.)\n",
    "gaussian_process = GaussianProcess(kernel_matern)\n",
    "\n",
    "gaussian_process.plot_with_samples(5, objective_function)\n",
    "plt.show()\n",
    "\n",
    "boundaries, = objective_function.boundaries\n",
    "x = np.linspace(*boundaries, 50).reshape((-1, 1))\n",
    "y = objective_function.evaluate(x).reshape((-1, 1))\n",
    "gaussian_process.initialise_dataset(x, y)\n",
    "\n",
    "gaussian_process.plot_with_samples(5, objective_function)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Marginal Likelihood (20 marks)\n",
    "\n",
    "So far, we have performed a gaussian process regression using some pre-defined kernel parameters. As you have seen in the previous examples, using inadequate parameters for the kernel may result in very poor performance regarding the regression. \n",
    "\n",
    "In order to find the optimal values for those parameters, we would like to maximise the *marginal likelihood* associated to the Gaussian Process.\n",
    "\n",
    "In reality, we will try to minimise the *negative log marginal likelihood*, which is easier to estimate and optimise.\n",
    "\n",
    "All the optimisation procedure is performed in the function `optimise_parameters()` of class `GaussianProcess`. This function is already written and does not require any further modification. To find the kernel parameters that minimise the *negative log marginal likelihood*, this `optimise_parameters()` function uses:\n",
    "* A function which computes the *negative log marginal likelihood* : `get_negative_log_marginal_likelihood(log_amplitude, log_length_scale, log_noise_scale)`\n",
    "* A function which computes its *gradient*: `get_gradient_negative_log_marginal_likelihood(log_amplitude, log_length_scale, log_noise_scale)`. This function is also already written and does not require any further modification.\n",
    "\n",
    "### Task\n",
    "\n",
    "Implement the method `get_negative_log_marginal_likelihood(log_amplitude, log_length_scale, log_noise_scale)` in `gaussian_process.py`.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "* `log_amplitude`: $\\log{\\sigma_f}$\n",
    "* `log_length_scale`: $\\log{l}$\n",
    "* `log_noise_scale`: $\\log{\\sigma_n}$\n",
    "\n",
    "### Returns\n",
    "\n",
    "This method returns the negative logarithm of the marginal likelihood corresponding to the gaussian process.\n",
    "\n",
    "$$ -\\log{p(y|X,\\theta)} = \\dfrac{1}{2}y^T K_\\theta^{-1} y + \\dfrac{1}{2} \\log{|K_\\theta|} + \\dfrac{n}{2} \\log{2\\pi} $$\n",
    "\n",
    "where $K_\\theta = k(X, X) + \\sigma_n^2 I$\n",
    "\n",
    "### Useful functions\n",
    "\n",
    "In addition to the previous shown functions and attributes, you may find the following ones useful in this task:\n",
    "* `set_kernel_parameters(log_amplitude: float, log_length_scale: float, log_noise_scale: float)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "Perform two Gaussian Process regressions as before: one with the gaussian kernel and one with the Matern Kernel.\n",
    "Which one is the best? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for your tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Implement metrics, to measure performance on test set (10 marks)\n",
    "\n",
    "Let's now implement a function to measure the performance of the optimised Gaussian Process on a test set. This way, we will be able to visualise how well it generalises to other data points.\n",
    "\n",
    "As we are performing a regression task, we can try to measure the **log predictive density**, which estimates the accuracy of future data drawn from the same distribution.\n",
    "\n",
    "### Task\n",
    "\n",
    "Implement the method `get_log_predictive_density(data_points_test, evaluations_test)` in file `gaussian_process.py`\n",
    "\n",
    "### Parameters\n",
    "\n",
    "* `data_points_test` ($X_{test}$) - It is a numpy array  in which all the rows $(x^{(i)}_{test})_{i=1..n}$ correspond to a data point at which the objective function can be evaluated. Its shape is $n$ x $p$ where: \n",
    "    * $n$ is the number of points.\n",
    "    * $p$ is the number of coordinates for each point (and the number of variables of the objective function).\n",
    "* `evaluations_test` ($y_{test}$) - The array of test data evaluations. It is a column vector whose $i^{th}$ element corresponds to the evaluation by the objective function of $x^{(i)}_{test}$.\n",
    "\n",
    "### Returns\n",
    "\n",
    "It returns the log predictive density $lpd$ calculated using the test set:\n",
    "\n",
    "$$lpd = \\sum_{i=1}^{N_{test}} \\log p(y^{(i)}_{test} | \\mathbf{x}^{(i)}_{test}, X_{train}, \\mathbf{y_{train}} )$$\n",
    "\n",
    "----\n",
    "\n",
    "Once this function is implemented, we can try to compute the log predictive dentisy for our initial dataset and our optimised matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_function = UnivariateObjectiveFunction(additional_gaussian_noise_std=0.5)\n",
    "\n",
    "from kernels.gaussian_kernel import GaussianKernel\n",
    "\n",
    "kernel_gaussian = GaussianKernel(-1., 0., -1.)\n",
    "gaussian_process = GaussianProcess(kernel_gaussian)\n",
    "\n",
    "boundaries, = objective_function.boundaries\n",
    "x_train = np.linspace(*boundaries, 50).reshape((-1, 1))\n",
    "y_train = objective_function.evaluate(x_train).reshape((-1, 1))\n",
    "\n",
    "x_test = np.linspace(*boundaries, 150).reshape((-1, 1))\n",
    "y_test = objective_function.evaluate(x_test).reshape((-1, 1))\n",
    "\n",
    "\n",
    "gaussian_process.initialise_dataset(x_train, y_train)\n",
    "print(f\"LPD Gaussian Process without marginal likelihood optimisation: {gaussian_process.get_log_predictive_density(x_test, y_test)}\")\n",
    "gaussian_process.plot(objective_function)\n",
    "\n",
    "print('-' * 50)\n",
    "gaussian_process.optimise_parameters(disp=False)\n",
    "\n",
    "print(f\"LPD optimised Gaussian Process: {gaussian_process.get_log_predictive_density(x_test, y_test)}\")\n",
    "gaussian_process.plot(objective_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: Acquisition Function (10 marks)\n",
    "\n",
    "Bayesian Optimisation is useful for optimising objective functions which are expensive to evaluate. To do so, it performs a regression on the objective function with a gaussian process and a limited number of points. Then it optimises a surrogate function which is easy to evaluate: the `acquisition function`. It's computed argmax tells us where to evaluate our black-box objective function at the next step.\n",
    "\n",
    "All the acquisition functions are represented by classes inheriting from the abstract class `AcquisitionFunction`. All of them are in the folder `acquisition_functions/`. The Lower-Confidence Bound acquisition function is already fully implemented.\n",
    "\n",
    "### Task\n",
    "\n",
    "The goal of this task is to implement another one: the Expected Improvement acquisition function. \n",
    "Complete the definition of the function `_evaluate(gaussian_process, data_points)` in `acquisition_functions/expected_improvement.py`\n",
    "\n",
    "### Parameters\n",
    "\n",
    "The function takes 2 input variables:\n",
    "* a Gaussian Process, as the acquisition functions depend on the mean and std predicted by the Gaussian Process\n",
    "* `data_points` - The array of training data (each row is a data point $(x_i)_{i=1..n}$ at which the acquisition function will be evaluated). Its shape is written $n$ x $p$.\n",
    "\n",
    "### Returns\n",
    "\n",
    "A numpy array of shape $n$ x $1$ (it is a vector column). The value at its $i^{th}$ position corresponds to the value taken by the acquisition function at $x_i$\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "Now, we have all the ingredients ready for performing a Bayesian Optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8: Bayesian Optimisation (10 marks)\n",
    "\n",
    "The Bayesian Optimisation algorithm, and all the functions which have something to do with it are grouped in the class `BayesianOptimisation` of the file `bayesian_optimisation.py`.\n",
    "\n",
    "Most of the code is already provided. All the algorithm is implemented in the `run` method. The only method which remains to be completed is the `_bayesian_optimisation_step` method (called in the main loop of `run`).\n",
    "\n",
    "### Task\n",
    "\n",
    "Complete the definition of the method `_bayesian_optimisation_step` in the file `bayesian_optimisation.py`.\n",
    "\n",
    "### Description\n",
    "\n",
    "* The single argument of the `_bayesian_optimisation_step` method corresponds to the last computed argmax of the acquisition function.\n",
    "* Update the gaussian process by using this new point.\n",
    "* Optimise the Gaussian Process\n",
    "* Compute and return the next point at which we will evaluate the objective function.\n",
    "\n",
    "### Some useful methods\n",
    "\n",
    "You may need to use the method `compute_arg_max` implemented in the `Acquisition Function` abstract class.\n",
    "\n",
    "-----\n",
    "\n",
    "You may find below some code for showing the detailed steps of a Bayesian Optimisation for some objective functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------\n",
    "# Bayesian Optimisation in 1 dimension\n",
    "#---------------\n",
    "\n",
    "from acquisition_functions.expected_improvement import ExpectedImprovement\n",
    "\n",
    "kernel = GaussianKernel(-1., -1., -1.)\n",
    "\n",
    "objective_function = objective_functions.univariate_objective_function.UnivariateObjectiveFunction()\n",
    "# objective_function = objective_functions.six_hump_camel.SixHumpCamelObjectiveFunction()\n",
    "\n",
    "# acquisition_function = LowerConfidenceBound(2.)\n",
    "acquisition_function = ExpectedImprovement()\n",
    "\n",
    "bayesian_optimisation = BayesianOptimisation(\n",
    "    kernel,\n",
    "    objective_function,\n",
    "    acquisition_function\n",
    ")\n",
    "\n",
    "number_initial_elements = 1\n",
    "print(f\"Initialising Dataset with {number_initial_elements} Initial Elements\")\n",
    "dataset = objective_function.get_random_initial_dataset(number_initial_elements)\n",
    "evaluations = objective_function.evaluate(dataset)\n",
    "\n",
    "number_steps_bayesian_optimisation = 25\n",
    "print(f\"Launching Bayesian Optimisation with {number_steps_bayesian_optimisation} Steps\")\n",
    "\n",
    "bo_generator = bayesian_optimisation.run(\n",
    "    number_steps=number_steps_bayesian_optimisation,\n",
    "    array_initial_dataset=dataset,\n",
    "    array_initial_objective_function_values=evaluations\n",
    ")\n",
    "\n",
    "for gp, aq, arg_max_aq in bo_generator:\n",
    "    print(objective_function.boundaries)\n",
    "    boundaries_x, = objective_function.boundaries\n",
    "    plt.xlim(boundaries_x)\n",
    "    plt.ylim(-3., 3.)\n",
    "\n",
    "    gp.plot(objective_function)\n",
    "    \n",
    "    plt.xlim(boundaries_x)\n",
    "    aq.plot(gp, objective_function, last_evaluated_point=arg_max_aq)\n",
    "\n",
    "print(f\"Best argmin found for the objective function: {bayesian_optimisation.get_best_data_point()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------\n",
    "# Bayesian Optimisation in 2 dimensions\n",
    "#---------------\n",
    "\n",
    "\n",
    "from acquisition_functions.expected_improvement import ExpectedImprovement\n",
    "\n",
    "kernel = GaussianKernel(0.5, np.log(1.), 1*np.log(1.))\n",
    "\n",
    "# objective_function = objective_functions.univariate_objective_function.UnivariateObjectiveFunction()\n",
    "objective_function = objective_functions.six_hump_camel.SixHumpCamelObjectiveFunction()\n",
    "\n",
    "# acquisition_function = LowerConfidenceBound(2.)\n",
    "acquisition_function = ExpectedImprovement()\n",
    "\n",
    "bayesian_optimisation = BayesianOptimisation(\n",
    "    kernel,\n",
    "    objective_function,\n",
    "    acquisition_function\n",
    ")\n",
    "\n",
    "number_initial_elements = 4\n",
    "print(f\"Initialising Dataset with {number_initial_elements} Initial Elements\")\n",
    "dataset = objective_function.get_random_initial_dataset(number_initial_elements)\n",
    "evaluations = objective_function.evaluate(dataset)\n",
    "\n",
    "number_steps_bayesian_optimisation = 25\n",
    "print(f\"Launching Bayesian Optimisation with {number_steps_bayesian_optimisation} Steps\")\n",
    "\n",
    "bo_generator = bayesian_optimisation.run(\n",
    "    number_steps=number_steps_bayesian_optimisation,\n",
    "    array_initial_dataset=dataset,\n",
    "    array_initial_objective_function_values=evaluations\n",
    ")\n",
    "\n",
    "for gp, aq, arg_max_aq in bo_generator:\n",
    "    print(objective_function.boundaries)\n",
    "    boundaries_x, boundaries_y = objective_function.boundaries\n",
    "    plt.xlim(boundaries_x)\n",
    "    plt.ylim(boundaries_y)\n",
    "    gp.plot(objective_function)\n",
    "    \n",
    "    plt.xlim(boundaries_x)\n",
    "    aq.plot(gp, objective_function, last_evaluated_point=arg_max_aq)\n",
    "\n",
    "print(f\"Best argmin found for the objective function: {bayesian_optimisation.get_best_data_point()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "What happens if there is not enough data provided at the Initialisation Step?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for your tests"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (co493-coursework-bayesian-optimisation)",
   "language": "python",
   "name": "pycharm-f9326eb3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}